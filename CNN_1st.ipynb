{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_width = 101\n",
    "img_height = 101\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid('on')\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# First layer\n",
    "model.add(Convolution2D(filters = 32, kernel_size = (3, 3), \n",
    "                        input_shape = (img_width, img_height, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Second layer\n",
    "model.add(Convolution2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC\n",
    "model.add(Dense(units = 5, activation = 'tanh'))\n",
    "\n",
    "# Output \n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and Compile model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image data\n",
    "train_data_dir = './data/training'\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.1, zoom_range = 0.2, horizontal_flip = True,\n",
    "                                  validation_split = 0.33)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1005 validated image filenames belonging to 2 classes.\n",
      "Found 495 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate image label dataframe\n",
    "traindf = pd.read_csv(\"./data/labels_training.csv\",dtype=str)\n",
    "def append_ext(fn):\n",
    "    return fn+\".tif\"\n",
    "traindf[\"id\"]=traindf[\"id\"].apply(append_ext)\n",
    "traindf = traindf.sample(frac=1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=train_data_dir, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=42,\n",
    "    batch_size = batch_size,\n",
    "    target_size=(img_width, img_height),\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=train_data_dir, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=42,\n",
    "    batch_size = 32,\n",
    "    target_size=(img_width, img_height),\n",
    "    class_mode='binary',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=31, validation_data=<keras.pre..., validation_steps=15, epochs=20)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - 4s 128ms/step - loss: 0.6560 - binary_accuracy: 0.6413 - val_loss: 0.6348 - val_binary_accuracy: 0.6646\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 4s 145ms/step - loss: 0.6409 - binary_accuracy: 0.6536 - val_loss: 0.6408 - val_binary_accuracy: 0.6674\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 5s 149ms/step - loss: 0.6049 - binary_accuracy: 0.6783 - val_loss: 0.6405 - val_binary_accuracy: 0.6587\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 5s 150ms/step - loss: 0.5686 - binary_accuracy: 0.7177 - val_loss: 0.5205 - val_binary_accuracy: 0.7171\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 4s 144ms/step - loss: 0.5832 - binary_accuracy: 0.6981 - val_loss: 0.5633 - val_binary_accuracy: 0.6803\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 5s 147ms/step - loss: 0.5426 - binary_accuracy: 0.7318 - val_loss: 0.6685 - val_binary_accuracy: 0.7408\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 4s 139ms/step - loss: 0.5193 - binary_accuracy: 0.7429 - val_loss: 0.4786 - val_binary_accuracy: 0.7343\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 4s 138ms/step - loss: 0.5374 - binary_accuracy: 0.7442 - val_loss: 0.5973 - val_binary_accuracy: 0.7171\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 4s 134ms/step - loss: 0.5217 - binary_accuracy: 0.7400 - val_loss: 0.6160 - val_binary_accuracy: 0.7322\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 4s 135ms/step - loss: 0.4704 - binary_accuracy: 0.7811 - val_loss: 0.5053 - val_binary_accuracy: 0.7603\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 4s 144ms/step - loss: 0.4653 - binary_accuracy: 0.7924 - val_loss: 0.5228 - val_binary_accuracy: 0.6955\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 5s 152ms/step - loss: 0.5299 - binary_accuracy: 0.7540 - val_loss: 0.6463 - val_binary_accuracy: 0.7473\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 4s 135ms/step - loss: 0.4529 - binary_accuracy: 0.8006 - val_loss: 0.5986 - val_binary_accuracy: 0.6263\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 4s 134ms/step - loss: 0.4894 - binary_accuracy: 0.7760 - val_loss: 0.5106 - val_binary_accuracy: 0.7257\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 4s 124ms/step - loss: 0.4416 - binary_accuracy: 0.8078 - val_loss: 0.4693 - val_binary_accuracy: 0.7667\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 4s 126ms/step - loss: 0.4680 - binary_accuracy: 0.7778 - val_loss: 0.4776 - val_binary_accuracy: 0.7322\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 4s 131ms/step - loss: 0.4245 - binary_accuracy: 0.8150 - val_loss: 0.5342 - val_binary_accuracy: 0.7375\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 5s 152ms/step - loss: 0.4517 - binary_accuracy: 0.8037 - val_loss: 0.6361 - val_binary_accuracy: 0.7559\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 4s 137ms/step - loss: 0.4171 - binary_accuracy: 0.8196 - val_loss: 0.5464 - val_binary_accuracy: 0.7322\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 4s 134ms/step - loss: 0.3944 - binary_accuracy: 0.8438 - val_loss: 0.4571 - val_binary_accuracy: 0.7365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14cb82710>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate fit generator\n",
    "model.fit_generator(train_generator, steps_per_epoch = train_generator.samples//batch_size, \n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = validation_generator.samples//batch_size,\n",
    "                    nb_epoch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-5ed2d455c237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#plot_roc(labels, score.ravel())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_class'"
     ]
    }
   ],
   "source": [
    "#score = model.predict_generator(validation_generator)\n",
    "#labels = validation_generator.classes\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = model.predict_generator(train_generator)\n",
    "labels = train_generator.classes\n",
    "#plot_roc(labels, score.ravel())\n",
    "auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "print(auc)\n",
    "print (classification_report(labels, score.ravel()>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 validated image filenames belonging to 2 classes.\n",
      "Found 558 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Retrain with all data\n",
    "\n",
    "# Get image data\n",
    "train_data_dir = './data/training'\n",
    "test_data_dir = './data/testing'\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.1, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Generate image label dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=train_data_dir, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=42,\n",
    "    batch_size = batch_size,\n",
    "    target_size=(img_width, img_height),\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/Cellar/ipython/7.8.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=46, epochs=30)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 0.6596 - accuracy: 0.6594\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 0.6475 - accuracy: 0.6628\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.6452 - accuracy: 0.6574\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 112ms/step - loss: 0.6335 - accuracy: 0.6628\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 113ms/step - loss: 0.6281 - accuracy: 0.6692\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.6130 - accuracy: 0.6626\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.6063 - accuracy: 0.6683\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 113ms/step - loss: 0.6089 - accuracy: 0.6535\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 113ms/step - loss: 0.5695 - accuracy: 0.6776\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 118ms/step - loss: 0.5756 - accuracy: 0.6540\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.5676 - accuracy: 0.6567\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 116ms/step - loss: 0.5707 - accuracy: 0.7023\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.5668 - accuracy: 0.7193\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 113ms/step - loss: 0.5532 - accuracy: 0.7473\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.5384 - accuracy: 0.7432\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 5s 114ms/step - loss: 0.5457 - accuracy: 0.7459\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 6s 125ms/step - loss: 0.5212 - accuracy: 0.7664\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 5s 115ms/step - loss: 0.5279 - accuracy: 0.7616\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 5s 116ms/step - loss: 0.5156 - accuracy: 0.7704\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 6s 125ms/step - loss: 0.4964 - accuracy: 0.7944\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 6s 121ms/step - loss: 0.4906 - accuracy: 0.7820\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 6s 130ms/step - loss: 0.4931 - accuracy: 0.7853\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 5s 118ms/step - loss: 0.4794 - accuracy: 0.7977\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 5s 118ms/step - loss: 0.4670 - accuracy: 0.8059\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 6s 123ms/step - loss: 0.4707 - accuracy: 0.7881\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 6s 124ms/step - loss: 0.4433 - accuracy: 0.8127\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 5s 115ms/step - loss: 0.4710 - accuracy: 0.7745\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 5s 111ms/step - loss: 0.4570 - accuracy: 0.8202\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 5s 112ms/step - loss: 0.4404 - accuracy: 0.8222\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 0.4513 - accuracy: 0.8154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x145772d10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reoptimize model on the full training set\n",
    "\n",
    "# Setup optimizer and Compile model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Generate fit generator\n",
    "model.fit_generator(train_generator, steps_per_epoch = train_generator.samples//batch_size,\n",
    "                    nb_epoch = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = './data/training/'\n",
    "dir_test_images   = './data/testing/'\n",
    "dir_train_labels  = './data/labels_training.csv'\n",
    "dir_test_ids      = './data/sample_submission.csv'\n",
    "\n",
    "# Load the test data and test the classifier\n",
    "test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "test_data = test_data/255\n",
    "test_scores    = model.predict_proba(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the predictions to a CSV file for upload to Kaggle\n",
    "submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores.ravel()})\n",
    "submission_file.to_csv('submission_CNN.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer = pd.read_csv('submission_PCA_SVM_3C.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.55051583],\n",
       "       [0.55051583, 1.        ]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(test_scores.ravel(),np.array(peer.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02277895]\n",
      " [0.40673482]\n",
      " [0.34235457]\n",
      " [0.61456823]\n",
      " [0.1727706 ]\n",
      " [0.08660366]\n",
      " [0.126175  ]\n",
      " [0.0432112 ]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.04241512]\n",
      " [0.06314329]\n",
      " [0.14860891]\n",
      " [0.10705954]\n",
      " [0.02988541]\n",
      " [0.04064206]\n",
      " [0.61456823]\n",
      " [0.06816328]\n",
      " [0.39856255]\n",
      " [0.0684965 ]\n",
      " [0.15086024]\n",
      " [0.05097397]\n",
      " [0.61456823]\n",
      " [0.00461176]\n",
      " [0.02062681]\n",
      " [0.04107146]\n",
      " [0.5419805 ]\n",
      " [0.0425634 ]\n",
      " [0.39531627]\n",
      " [0.12381522]\n",
      " [0.02820242]\n",
      " [0.03623557]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.03285847]\n",
      " [0.61456823]\n",
      " [0.16615933]\n",
      " [0.01679355]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.4765532 ]\n",
      " [0.38803717]\n",
      " [0.03237739]\n",
      " [0.13267504]\n",
      " [0.02393799]\n",
      " [0.00575777]\n",
      " [0.18776496]\n",
      " [0.61456823]\n",
      " [0.04360319]\n",
      " [0.61456823]\n",
      " [0.10414106]\n",
      " [0.2081161 ]\n",
      " [0.02468728]\n",
      " [0.61456823]\n",
      " [0.1462597 ]\n",
      " [0.09362742]\n",
      " [0.61456823]\n",
      " [0.05050301]\n",
      " [0.11330345]\n",
      " [0.04345893]\n",
      " [0.13870475]\n",
      " [0.04093717]\n",
      " [0.22369507]\n",
      " [0.0422663 ]\n",
      " [0.03117658]\n",
      " [0.00977774]\n",
      " [0.05777572]\n",
      " [0.0952001 ]\n",
      " [0.61456823]\n",
      " [0.5867135 ]\n",
      " [0.61456823]\n",
      " [0.5135095 ]\n",
      " [0.13204645]\n",
      " [0.01472677]\n",
      " [0.08765404]\n",
      " [0.18490301]\n",
      " [0.61456823]\n",
      " [0.2622351 ]\n",
      " [0.08195833]\n",
      " [0.03688729]\n",
      " [0.03290027]\n",
      " [0.22546011]\n",
      " [0.33535227]\n",
      " [0.61456823]\n",
      " [0.37740493]\n",
      " [0.0518673 ]\n",
      " [0.14509028]\n",
      " [0.06091162]\n",
      " [0.07829695]\n",
      " [0.27802396]\n",
      " [0.00358472]\n",
      " [0.11855396]\n",
      " [0.60146654]\n",
      " [0.06147776]\n",
      " [0.03106789]\n",
      " [0.02370088]\n",
      " [0.52144265]\n",
      " [0.27750924]\n",
      " [0.07448287]\n",
      " [0.07210772]\n",
      " [0.13014904]\n",
      " [0.01099594]\n",
      " [0.40205318]\n",
      " [0.61456823]\n",
      " [0.03204712]\n",
      " [0.02398059]\n",
      " [0.2516605 ]\n",
      " [0.00974232]\n",
      " [0.07491129]\n",
      " [0.02053159]\n",
      " [0.04552205]\n",
      " [0.572369  ]\n",
      " [0.09420777]\n",
      " [0.601231  ]\n",
      " [0.02924934]\n",
      " [0.5472243 ]\n",
      " [0.27487442]\n",
      " [0.61456823]\n",
      " [0.08621928]\n",
      " [0.30988985]\n",
      " [0.13413663]\n",
      " [0.08343105]\n",
      " [0.04605734]\n",
      " [0.05401294]\n",
      " [0.08024918]\n",
      " [0.22439441]\n",
      " [0.16323555]\n",
      " [0.19276775]\n",
      " [0.07697526]\n",
      " [0.2090052 ]\n",
      " [0.0603493 ]\n",
      " [0.2295846 ]\n",
      " [0.2224984 ]\n",
      " [0.28817666]\n",
      " [0.00866732]\n",
      " [0.61456823]\n",
      " [0.20717771]\n",
      " [0.61456823]\n",
      " [0.25848937]\n",
      " [0.61456823]\n",
      " [0.02900502]\n",
      " [0.01401433]\n",
      " [0.18978201]\n",
      " [0.13349515]\n",
      " [0.03835733]\n",
      " [0.03725678]\n",
      " [0.61456823]\n",
      " [0.02611591]\n",
      " [0.61456823]\n",
      " [0.12279344]\n",
      " [0.07328784]\n",
      " [0.14982769]\n",
      " [0.18737446]\n",
      " [0.11629828]\n",
      " [0.43468848]\n",
      " [0.03504867]\n",
      " [0.3724387 ]\n",
      " [0.16400208]\n",
      " [0.02876853]\n",
      " [0.53440076]\n",
      " [0.4894062 ]\n",
      " [0.61456823]\n",
      " [0.03261314]\n",
      " [0.1262979 ]\n",
      " [0.02478393]\n",
      " [0.61456823]\n",
      " [0.21322063]\n",
      " [0.61456823]\n",
      " [0.0727234 ]\n",
      " [0.04316609]\n",
      " [0.02732554]\n",
      " [0.05061405]\n",
      " [0.12796296]\n",
      " [0.14898783]\n",
      " [0.05577832]\n",
      " [0.61456823]\n",
      " [0.46969005]\n",
      " [0.00629752]\n",
      " [0.09916996]\n",
      " [0.4458927 ]\n",
      " [0.02705132]\n",
      " [0.05685832]\n",
      " [0.11055201]\n",
      " [0.4789699 ]\n",
      " [0.04701186]\n",
      " [0.04376188]\n",
      " [0.3036184 ]\n",
      " [0.37767988]\n",
      " [0.61456823]\n",
      " [0.19980903]\n",
      " [0.02282799]\n",
      " [0.6089417 ]\n",
      " [0.07248561]\n",
      " [0.31651482]\n",
      " [0.11066893]\n",
      " [0.15591303]\n",
      " [0.61456823]\n",
      " [0.02102158]\n",
      " [0.01941571]\n",
      " [0.30263758]\n",
      " [0.61456823]\n",
      " [0.2501197 ]\n",
      " [0.49568692]\n",
      " [0.36503175]\n",
      " [0.0421817 ]\n",
      " [0.20407464]\n",
      " [0.00568133]\n",
      " [0.13858388]\n",
      " [0.01136928]\n",
      " [0.5607289 ]\n",
      " [0.0424782 ]\n",
      " [0.3892736 ]\n",
      " [0.3538661 ]\n",
      " [0.0486397 ]\n",
      " [0.00866895]\n",
      " [0.09238338]\n",
      " [0.23390473]\n",
      " [0.15002929]\n",
      " [0.16872294]\n",
      " [0.61456823]\n",
      " [0.01325984]\n",
      " [0.11707373]\n",
      " [0.01892811]\n",
      " [0.17767525]\n",
      " [0.17407018]\n",
      " [0.4141765 ]\n",
      " [0.03726967]\n",
      " [0.10193367]\n",
      " [0.5716841 ]\n",
      " [0.14606081]\n",
      " [0.3668705 ]\n",
      " [0.0869085 ]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.12771904]\n",
      " [0.51061726]\n",
      " [0.13879047]\n",
      " [0.0439547 ]\n",
      " [0.28109765]\n",
      " [0.3566069 ]\n",
      " [0.5993339 ]\n",
      " [0.1312736 ]\n",
      " [0.01809592]\n",
      " [0.08734503]\n",
      " [0.05514277]\n",
      " [0.49439362]\n",
      " [0.10587283]\n",
      " [0.08645712]\n",
      " [0.17721042]\n",
      " [0.08739647]\n",
      " [0.05398564]\n",
      " [0.16007408]\n",
      " [0.01807815]\n",
      " [0.61456823]\n",
      " [0.41246706]\n",
      " [0.4738356 ]\n",
      " [0.02839889]\n",
      " [0.12235101]\n",
      " [0.40027764]\n",
      " [0.61456823]\n",
      " [0.01409097]\n",
      " [0.07148761]\n",
      " [0.02739237]\n",
      " [0.6098137 ]\n",
      " [0.01252877]\n",
      " [0.01741735]\n",
      " [0.02152131]\n",
      " [0.33253527]\n",
      " [0.01683277]\n",
      " [0.581768  ]\n",
      " [0.34500635]\n",
      " [0.03309573]\n",
      " [0.07787964]\n",
      " [0.44412744]\n",
      " [0.05865459]\n",
      " [0.0663036 ]\n",
      " [0.3893422 ]\n",
      " [0.01231903]\n",
      " [0.4778247 ]\n",
      " [0.61456823]\n",
      " [0.5301451 ]\n",
      " [0.40103668]\n",
      " [0.04004452]\n",
      " [0.33351898]\n",
      " [0.61456823]\n",
      " [0.05762375]\n",
      " [0.50174004]\n",
      " [0.02990999]\n",
      " [0.04215389]\n",
      " [0.02852006]\n",
      " [0.04080527]\n",
      " [0.26752165]\n",
      " [0.01082131]\n",
      " [0.0176712 ]\n",
      " [0.23997626]\n",
      " [0.01861664]\n",
      " [0.4684309 ]\n",
      " [0.0715305 ]\n",
      " [0.07770918]\n",
      " [0.13599019]\n",
      " [0.11473279]\n",
      " [0.153794  ]\n",
      " [0.10580523]\n",
      " [0.1351935 ]\n",
      " [0.01986787]\n",
      " [0.43951464]\n",
      " [0.07841696]\n",
      " [0.1201225 ]\n",
      " [0.27937642]\n",
      " [0.42049366]\n",
      " [0.61456823]\n",
      " [0.13064653]\n",
      " [0.17589894]\n",
      " [0.04205988]\n",
      " [0.01459422]\n",
      " [0.03868594]\n",
      " [0.07834872]\n",
      " [0.61456823]\n",
      " [0.14989431]\n",
      " [0.06754015]\n",
      " [0.06449924]\n",
      " [0.0492595 ]\n",
      " [0.00617966]\n",
      " [0.09030191]\n",
      " [0.15496561]\n",
      " [0.60532993]\n",
      " [0.54314446]\n",
      " [0.2226276 ]\n",
      " [0.61456823]\n",
      " [0.02008803]\n",
      " [0.61456823]\n",
      " [0.01334514]\n",
      " [0.05744136]\n",
      " [0.05146416]\n",
      " [0.13572095]\n",
      " [0.02871631]\n",
      " [0.24763696]\n",
      " [0.13216574]\n",
      " [0.0686332 ]\n",
      " [0.12852846]\n",
      " [0.01583386]\n",
      " [0.02258086]\n",
      " [0.61456823]\n",
      " [0.06547368]\n",
      " [0.43777946]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.06217007]\n",
      " [0.09658373]\n",
      " [0.49707925]\n",
      " [0.1763279 ]\n",
      " [0.04753933]\n",
      " [0.0059641 ]\n",
      " [0.61456823]\n",
      " [0.44913617]\n",
      " [0.29580235]\n",
      " [0.61456823]\n",
      " [0.03341106]\n",
      " [0.03050652]\n",
      " [0.06903531]\n",
      " [0.44962883]\n",
      " [0.10177433]\n",
      " [0.03463252]\n",
      " [0.61456823]\n",
      " [0.19866964]\n",
      " [0.5012122 ]\n",
      " [0.12811272]\n",
      " [0.27760842]\n",
      " [0.61456823]\n",
      " [0.04785393]\n",
      " [0.08788498]\n",
      " [0.03440133]\n",
      " [0.47356936]\n",
      " [0.07360206]\n",
      " [0.12586115]\n",
      " [0.01347136]\n",
      " [0.0636981 ]\n",
      " [0.04283984]\n",
      " [0.05015492]\n",
      " [0.13009198]\n",
      " [0.12833662]\n",
      " [0.06528316]\n",
      " [0.16942114]\n",
      " [0.50140566]\n",
      " [0.49416226]\n",
      " [0.03586075]\n",
      " [0.03495874]\n",
      " [0.04262533]\n",
      " [0.02016276]\n",
      " [0.19905357]\n",
      " [0.44193318]\n",
      " [0.61456823]\n",
      " [0.0258895 ]\n",
      " [0.07966381]\n",
      " [0.00978735]\n",
      " [0.03802651]\n",
      " [0.26906604]\n",
      " [0.3704239 ]\n",
      " [0.21506388]\n",
      " [0.1141383 ]\n",
      " [0.45708138]\n",
      " [0.20023474]\n",
      " [0.21630698]\n",
      " [0.0681323 ]\n",
      " [0.26912558]\n",
      " [0.33398202]\n",
      " [0.11323683]\n",
      " [0.02712232]\n",
      " [0.09434281]\n",
      " [0.19818826]\n",
      " [0.01187843]\n",
      " [0.00687487]\n",
      " [0.61456823]\n",
      " [0.36940613]\n",
      " [0.03900512]\n",
      " [0.09880201]\n",
      " [0.61456823]\n",
      " [0.18726386]\n",
      " [0.4063272 ]\n",
      " [0.590201  ]\n",
      " [0.01427636]\n",
      " [0.35194993]\n",
      " [0.03944959]\n",
      " [0.10413599]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.00893203]\n",
      " [0.12677741]\n",
      " [0.0062073 ]\n",
      " [0.08312834]\n",
      " [0.09981017]\n",
      " [0.08350534]\n",
      " [0.0163274 ]\n",
      " [0.00902147]\n",
      " [0.01106592]\n",
      " [0.32675448]\n",
      " [0.20892371]\n",
      " [0.0112377 ]\n",
      " [0.61456823]\n",
      " [0.04444535]\n",
      " [0.52342933]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.05432314]\n",
      " [0.2819586 ]\n",
      " [0.61456823]\n",
      " [0.21931998]\n",
      " [0.04203751]\n",
      " [0.01363147]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.55571145]\n",
      " [0.0407285 ]\n",
      " [0.05275289]\n",
      " [0.61456823]\n",
      " [0.61456823]\n",
      " [0.2706293 ]\n",
      " [0.1386177 ]\n",
      " [0.05650229]\n",
      " [0.0820713 ]\n",
      " [0.03303997]\n",
      " [0.00655761]\n",
      " [0.05386194]\n",
      " [0.08753958]\n",
      " [0.31808427]\n",
      " [0.06797455]\n",
      " [0.58361846]\n",
      " [0.5413751 ]\n",
      " [0.01987395]\n",
      " [0.61456823]\n",
      " [0.20607631]\n",
      " [0.61456823]\n",
      " [0.30305472]\n",
      " [0.09494305]\n",
      " [0.09729934]\n",
      " [0.02486076]\n",
      " [0.04599476]\n",
      " [0.563754  ]\n",
      " [0.14977135]\n",
      " [0.6108333 ]\n",
      " [0.09857018]\n",
      " [0.24620719]\n",
      " [0.61456823]\n",
      " [0.5653342 ]\n",
      " [0.0885743 ]\n",
      " [0.04897875]\n",
      " [0.2790154 ]\n",
      " [0.02243399]\n",
      " [0.03046164]\n",
      " [0.29109597]\n",
      " [0.01605172]\n",
      " [0.17312978]\n",
      " [0.00290552]\n",
      " [0.0266868 ]\n",
      " [0.06842869]]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
