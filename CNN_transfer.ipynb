{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import PIL\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_width = 101\n",
    "img_height = 101\n",
    "batch_size = 64\n",
    "keras = tf.keras\n",
    "\n",
    "IMG_SHAPE = (img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid('on')\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids\n",
    "    \n",
    "def cv_performance_assessment(X,y,k,clf):\n",
    "    '''Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        # Train the classifier\n",
    "        X_train_features = X_train\n",
    "        clf              = clf.fit(X_train_features,y_train)\n",
    "        \n",
    "        # Test the classifier on the validation data for this fold\n",
    "        X_val_features   = X_val\n",
    "        cpred            = clf.predict_proba(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "    return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = './data/training/'\n",
    "dir_test_images   = './data/testing/'\n",
    "dir_train_labels  = './data/labels_training.csv'\n",
    "dir_test_ids      = './data/sample_submission.csv'\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, vertical_flip = True, \n",
    "                                   horizontal_flip=True, channel_shift_range=50.0,\n",
    "                                   rotation_range = 30, shear_range = 10.0,\n",
    "                                   validation_split = 0.15)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1275 validated image filenames belonging to 2 classes.\n",
      "Found 225 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate image label dataframe\n",
    "# Dont run this\n",
    "traindf = pd.read_csv(\"./data/labels_training.csv\",dtype=str)\n",
    "def append_ext(fn):\n",
    "    return fn+\".tif\"\n",
    "traindf[\"id\"]=traindf[\"id\"].apply(append_ext)\n",
    "traindf = traindf.sample(frac=1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=train_data_dir, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=0,\n",
    "    batch_size = batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=train_data_dir, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=0,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle = False,\n",
    "    batch_size = 225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try extracting features to do SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = []\n",
    "for i in range(1500):\n",
    "    training_features.append(np.array(base_model(train_generator.next())).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"training_features_InceptionV3.csv\", training_features, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970727896910294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93       995\n",
      "           1       0.91      0.81      0.86       505\n",
      "\n",
      "    accuracy                           0.91      1500\n",
      "   macro avg       0.91      0.89      0.90      1500\n",
      "weighted avg       0.91      0.91      0.91      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "labels = train_generator.classes\n",
    "clf = svm.SVC(kernel='rbf', degree = 3, probability=True, C=0.1, gamma='scale')\n",
    "clf.fit(training_features, labels)\n",
    "\n",
    "# Primary evaluation\n",
    "score = clf.predict_proba(training_features)\n",
    "auc = metrics.roc_auc_score(labels, score[:,1])\n",
    "print(auc)\n",
    "print (classification_report(labels, score[:,1]>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', probability=True, C=1, gamma = 'scale')\n",
    "# Perform cross validated performance assessment\n",
    "prediction_scores = cv_performance_assessment(np.array(training_features),np.array(labels),5,clf)\n",
    "\n",
    "# Compute and plot the ROC curves\n",
    "plot_roc(labels, prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 558 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    batch_size=1\n",
    ")\n",
    "testing_features = []\n",
    "for i in range(558):\n",
    "    testing_features.append(np.array(base_model(test_generator.next())).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf', probability=True, C=1, gamma = 'scale')\n",
    "clf.fit(training_features, labels)\n",
    "\n",
    "score = clf.predict_proba(testing_features)\n",
    "dir_test_ids      = './data/sample_submission.csv'\n",
    "dir_test_images   = './data/testing/'\n",
    "test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "# Save the predictions to a CSV file for upload to Kaggle\n",
    "submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  score[:,1]})\n",
    "submission_file.to_csv('Inception_SVM2.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_mod(lr, weight, name, epoch = 100):\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # add a logistic layer -- let's say we have 200 classes\n",
    "    predictions = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    #for layer in base_model.layers[1:4]:\n",
    "    #    layer.trainable = True\n",
    "    \n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999), \n",
    "                  loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    filename = 'D:\\\\MIDS\\\\RedTeam\\\\' +name+ '.csv'\n",
    "    csv_logger = CSVLogger(filename, append=True, separator=';')\n",
    "    \n",
    "    model.fit_generator(train_generator, validation_data=validation_generator ,\n",
    "                        epochs=epoch, class_weight=weight, callbacks=[csv_logger])\n",
    "    \n",
    "    score = model.predict(train_generator)\n",
    "    labels = train_generator.classes\n",
    "    auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "    return (model, score, labels, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_mod2(lr, weight, name, epoch = 100):\n",
    "    # create the base pre-trained model\n",
    "    \n",
    "    model = Sequential()\n",
    "    # First layer\n",
    "    model.add(Convolution2D(filters = 8, kernel_size = (6, 6), strides = 1, \n",
    "                            input_shape = IMG_SHAPE, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "\n",
    "    # Second layer\n",
    "    model.add(Convolution2D(12, kernel_size = (6, 6), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Convolution2D(16, kernel_size = (4, 4), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # FC\n",
    "    model.add(Dense(units = 16, activation = 'sigmoid'))\n",
    "    \n",
    "    # Output \n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999), \n",
    "                  loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    filename = 'D:\\\\MIDS\\\\RedTeam\\\\' +name+ '.csv'\n",
    "    csv_logger = CSVLogger(filename, append=True, separator=';')\n",
    "    \n",
    "    model.fit_generator(train_generator, validation_data=validation_generator ,\n",
    "                        epochs=epoch, class_weight=weight, callbacks=[csv_logger])\n",
    "    \n",
    "    score = model.predict(train_generator)\n",
    "    labels = train_generator.classes\n",
    "    auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "    return (model, score, labels, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 0.7115 - auc_13: 0.5405 - val_loss: 0.6551 - val_auc_13: 0.5576\n",
      "Epoch 2/60\n",
      "20/20 [==============================] - 14s 688ms/step - loss: 0.6370 - auc_13: 0.6040 - val_loss: 0.6484 - val_auc_13: 0.6324\n",
      "Epoch 3/60\n",
      "20/20 [==============================] - 14s 698ms/step - loss: 0.6135 - auc_13: 0.6510 - val_loss: 0.7147 - val_auc_13: 0.6630\n",
      "Epoch 4/60\n",
      "20/20 [==============================] - 14s 696ms/step - loss: 0.6069 - auc_13: 0.6768 - val_loss: 0.6610 - val_auc_13: 0.6781\n",
      "Epoch 5/60\n",
      "20/20 [==============================] - 14s 698ms/step - loss: 0.5725 - auc_13: 0.6890 - val_loss: 0.8273 - val_auc_13: 0.6886\n",
      "Epoch 6/60\n",
      "20/20 [==============================] - 14s 698ms/step - loss: 0.5258 - auc_13: 0.7034 - val_loss: 0.8598 - val_auc_13: 0.7038\n",
      "Epoch 7/60\n",
      "20/20 [==============================] - 14s 697ms/step - loss: 0.5524 - auc_13: 0.7107 - val_loss: 0.8674 - val_auc_13: 0.7093\n",
      "Epoch 8/60\n",
      "20/20 [==============================] - 14s 700ms/step - loss: 0.5542 - auc_13: 0.7147 - val_loss: 1.6023 - val_auc_13: 0.7124\n",
      "Epoch 9/60\n",
      "20/20 [==============================] - 14s 698ms/step - loss: 0.5206 - auc_13: 0.7184 - val_loss: 0.7149 - val_auc_13: 0.7210\n",
      "Epoch 10/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.5081 - auc_13: 0.7273 - val_loss: 0.7607 - val_auc_13: 0.7286\n",
      "Epoch 11/60\n",
      "20/20 [==============================] - 14s 699ms/step - loss: 0.4700 - auc_13: 0.7345 - val_loss: 0.7689 - val_auc_13: 0.7372\n",
      "Epoch 12/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.4968 - auc_13: 0.7422 - val_loss: 0.7963 - val_auc_13: 0.7426\n",
      "Epoch 13/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.4811 - auc_13: 0.7473 - val_loss: 1.0594 - val_auc_13: 0.7458\n",
      "Epoch 14/60\n",
      "20/20 [==============================] - 14s 698ms/step - loss: 0.4625 - auc_13: 0.7502 - val_loss: 1.0947 - val_auc_13: 0.7491\n",
      "Epoch 15/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.5041 - auc_13: 0.7521 - val_loss: 1.0044 - val_auc_13: 0.7502\n",
      "Epoch 16/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.4720 - auc_13: 0.7539 - val_loss: 1.0036 - val_auc_13: 0.7527\n",
      "Epoch 17/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.4594 - auc_13: 0.7565 - val_loss: 0.9257 - val_auc_13: 0.7562\n",
      "Epoch 18/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.4704 - auc_13: 0.7593 - val_loss: 1.1253 - val_auc_13: 0.7576\n",
      "Epoch 19/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.4546 - auc_13: 0.7603 - val_loss: 1.0112 - val_auc_13: 0.7598\n",
      "Epoch 20/60\n",
      "20/20 [==============================] - 14s 697ms/step - loss: 0.4551 - auc_13: 0.7631 - val_loss: 1.2346 - val_auc_13: 0.7631\n",
      "Epoch 21/60\n",
      "20/20 [==============================] - 14s 699ms/step - loss: 0.4854 - auc_13: 0.7649 - val_loss: 0.8572 - val_auc_13: 0.7647\n",
      "Epoch 22/60\n",
      "20/20 [==============================] - 14s 700ms/step - loss: 0.4512 - auc_13: 0.7674 - val_loss: 0.7086 - val_auc_13: 0.7683\n",
      "Epoch 23/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.4529 - auc_13: 0.7705 - val_loss: 0.6681 - val_auc_13: 0.7718\n",
      "Epoch 24/60\n",
      "20/20 [==============================] - 14s 699ms/step - loss: 0.4652 - auc_13: 0.7739 - val_loss: 0.7457 - val_auc_13: 0.7742\n",
      "Epoch 25/60\n",
      "20/20 [==============================] - 14s 699ms/step - loss: 0.4293 - auc_13: 0.7764 - val_loss: 0.6741 - val_auc_13: 0.7777\n",
      "Epoch 26/60\n",
      "20/20 [==============================] - 14s 709ms/step - loss: 0.4365 - auc_13: 0.7799 - val_loss: 0.9440 - val_auc_13: 0.7796\n",
      "Epoch 27/60\n",
      "20/20 [==============================] - 14s 715ms/step - loss: 0.3947 - auc_13: 0.7818 - val_loss: 0.7235 - val_auc_13: 0.7830\n",
      "Epoch 28/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.3895 - auc_13: 0.7857 - val_loss: 1.9399 - val_auc_13: 0.7850\n",
      "Epoch 29/60\n",
      "20/20 [==============================] - 15s 740ms/step - loss: 0.4020 - auc_13: 0.7870 - val_loss: 0.8205 - val_auc_13: 0.7877\n",
      "Epoch 30/60\n",
      "20/20 [==============================] - 15s 749ms/step - loss: 0.4098 - auc_13: 0.7897 - val_loss: 2.1888 - val_auc_13: 0.7886\n",
      "Epoch 31/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.3888 - auc_13: 0.7904 - val_loss: 0.6939 - val_auc_13: 0.7915\n",
      "Epoch 32/60\n",
      "20/20 [==============================] - 14s 705ms/step - loss: 0.3825 - auc_13: 0.7937 - val_loss: 1.1411 - val_auc_13: 0.7932\n",
      "Epoch 33/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.3803 - auc_13: 0.7950 - val_loss: 1.3155 - val_auc_13: 0.7955\n",
      "Epoch 34/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.3562 - auc_13: 0.7975 - val_loss: 1.0295 - val_auc_13: 0.7984\n",
      "Epoch 35/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.4503 - auc_13: 0.7996 - val_loss: 0.8065 - val_auc_13: 0.8000\n",
      "Epoch 36/60\n",
      "20/20 [==============================] - 14s 706ms/step - loss: 0.4154 - auc_13: 0.8012 - val_loss: 0.5010 - val_auc_13: 0.8024\n",
      "Epoch 37/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.3744 - auc_13: 0.8040 - val_loss: 1.0773 - val_auc_13: 0.8037\n",
      "Epoch 38/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.3309 - auc_13: 0.8057 - val_loss: 0.7139 - val_auc_13: 0.8068\n",
      "Epoch 39/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.3505 - auc_13: 0.8085 - val_loss: 1.0558 - val_auc_13: 0.8084\n",
      "Epoch 40/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.2813 - auc_13: 0.8106 - val_loss: 0.8133 - val_auc_13: 0.8115\n",
      "Epoch 41/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2960 - auc_13: 0.8136 - val_loss: 0.5033 - val_auc_13: 0.8153\n",
      "Epoch 42/60\n",
      "20/20 [==============================] - 14s 706ms/step - loss: 0.2830 - auc_13: 0.8172 - val_loss: 0.5389 - val_auc_13: 0.8190\n",
      "Epoch 43/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.3102 - auc_13: 0.8206 - val_loss: 1.2395 - val_auc_13: 0.8202\n",
      "Epoch 44/60\n",
      "20/20 [==============================] - 14s 705ms/step - loss: 0.3170 - auc_13: 0.8216 - val_loss: 0.4804 - val_auc_13: 0.8231\n",
      "Epoch 45/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.3312 - auc_13: 0.8245 - val_loss: 0.4330 - val_auc_13: 0.8258\n",
      "Epoch 46/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.3210 - auc_13: 0.8272 - val_loss: 0.4954 - val_auc_13: 0.8284\n",
      "Epoch 47/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2856 - auc_13: 0.8300 - val_loss: 0.8017 - val_auc_13: 0.8306\n",
      "Epoch 48/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2751 - auc_13: 0.8320 - val_loss: 0.8150 - val_auc_13: 0.8327\n",
      "Epoch 49/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2899 - auc_13: 0.8342 - val_loss: 0.5916 - val_auc_13: 0.8353\n",
      "Epoch 50/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.2846 - auc_13: 0.8367 - val_loss: 0.3700 - val_auc_13: 0.8381\n",
      "Epoch 51/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.2854 - auc_13: 0.8395 - val_loss: 0.3532 - val_auc_13: 0.8410\n",
      "Epoch 52/60\n",
      "20/20 [==============================] - 14s 703ms/step - loss: 0.2766 - auc_13: 0.8423 - val_loss: 0.4138 - val_auc_13: 0.8435\n",
      "Epoch 53/60\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.2902 - auc_13: 0.8447 - val_loss: 0.9677 - val_auc_13: 0.8453\n",
      "Epoch 54/60\n",
      "20/20 [==============================] - 15s 725ms/step - loss: 0.2581 - auc_13: 0.8465 - val_loss: 0.5761 - val_auc_13: 0.8475\n",
      "Epoch 55/60\n",
      "20/20 [==============================] - 14s 705ms/step - loss: 0.2483 - auc_13: 0.8488 - val_loss: 0.3455 - val_auc_13: 0.8502\n",
      "Epoch 56/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2664 - auc_13: 0.8514 - val_loss: 0.4227 - val_auc_13: 0.8525\n",
      "Epoch 57/60\n",
      "20/20 [==============================] - 14s 704ms/step - loss: 0.2367 - auc_13: 0.8537 - val_loss: 0.4198 - val_auc_13: 0.8548\n",
      "Epoch 58/60\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 0.2898 - auc_13: 0.8559 - val_loss: 0.4315 - val_auc_13: 0.8568\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 14s 699ms/step - loss: 0.3012 - auc_13: 0.8577 - val_loss: 0.8795 - val_auc_13: 0.8580\n",
      "Epoch 60/60\n",
      "20/20 [==============================] - 14s 700ms/step - loss: 0.3105 - auc_13: 0.8587 - val_loss: 0.4274 - val_auc_13: 0.8595\n"
     ]
    }
   ],
   "source": [
    "model, s, l, a = CNN_mod2(0.005, {0:1., 1:1.2}, name = 'Try_to_overfit2', epoch = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 15s 738ms/step - loss: 0.2979 - auc_13: 0.8756 - val_loss: 0.4050 - val_auc_13: 0.8761\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 0.2373 - auc_13: 0.8769 - val_loss: 0.3057 - val_auc_13: 0.8777\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 15s 726ms/step - loss: 0.2282 - auc_13: 0.8785 - val_loss: 0.5763 - val_auc_13: 0.8790\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 14s 725ms/step - loss: 0.2447 - auc_13: 0.8797 - val_loss: 0.3381 - val_auc_13: 0.8804\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 15s 726ms/step - loss: 0.2290 - auc_13: 0.8811 - val_loss: 0.4294 - val_auc_13: 0.8818\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 15s 732ms/step - loss: 0.2267 - auc_13: 0.8824 - val_loss: 0.3841 - val_auc_13: 0.8832\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 14s 725ms/step - loss: 0.3028 - auc_13: 0.8837 - val_loss: 1.7441 - val_auc_13: 0.8832\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 15s 728ms/step - loss: 0.2742 - auc_13: 0.8837 - val_loss: 0.3587 - val_auc_13: 0.8843\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 15s 729ms/step - loss: 0.2406 - auc_13: 0.8848 - val_loss: 0.2850 - val_auc_13: 0.8855\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 0.2120 - auc_13: 0.8862 - val_loss: 0.6197 - val_auc_13: 0.8867\n",
      "0.9032144118685976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.60      0.73       149\n",
      "           1       0.54      0.92      0.68        76\n",
      "\n",
      "    accuracy                           0.71       225\n",
      "   macro avg       0.74      0.76      0.71       225\n",
      "weighted avg       0.80      0.71      0.72       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, validation_data=validation_generator ,\n",
    "                        epochs=10, class_weight={0:1., 1:1.25})\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = model.predict(validation_generator)\n",
    "labels = validation_generator.classes\n",
    "#plot_roc(labels, score.ravel())\n",
    "auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "print(auc)\n",
    "print (classification_report(labels, score.ravel()>=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbackobj = tf.compat.v1.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "#          write_graph=True, write_images=True, write_grads = True, update_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv2d_95\n",
      "2 batch_normalization_95\n",
      "3 activation_95\n",
      "4 conv2d_96\n",
      "5 batch_normalization_96\n",
      "6 activation_96\n",
      "7 conv2d_97\n",
      "8 batch_normalization_97\n",
      "9 activation_97\n",
      "10 max_pooling2d_5\n",
      "11 conv2d_98\n",
      "12 batch_normalization_98\n",
      "13 activation_98\n",
      "14 conv2d_99\n",
      "15 batch_normalization_99\n",
      "16 activation_99\n",
      "17 max_pooling2d_6\n",
      "18 conv2d_103\n",
      "19 batch_normalization_103\n",
      "20 activation_103\n",
      "21 conv2d_101\n",
      "22 conv2d_104\n",
      "23 batch_normalization_101\n",
      "24 batch_normalization_104\n",
      "25 activation_101\n",
      "26 activation_104\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_100\n",
      "29 conv2d_102\n",
      "30 conv2d_105\n",
      "31 conv2d_106\n",
      "32 batch_normalization_100\n",
      "33 batch_normalization_102\n",
      "34 batch_normalization_105\n",
      "35 batch_normalization_106\n",
      "36 activation_100\n",
      "37 activation_102\n",
      "38 activation_105\n",
      "39 activation_106\n",
      "40 mixed0\n",
      "41 conv2d_110\n",
      "42 batch_normalization_110\n",
      "43 activation_110\n",
      "44 conv2d_108\n",
      "45 conv2d_111\n",
      "46 batch_normalization_108\n",
      "47 batch_normalization_111\n",
      "48 activation_108\n",
      "49 activation_111\n",
      "50 average_pooling2d_11\n",
      "51 conv2d_107\n",
      "52 conv2d_109\n",
      "53 conv2d_112\n",
      "54 conv2d_113\n",
      "55 batch_normalization_107\n",
      "56 batch_normalization_109\n",
      "57 batch_normalization_112\n",
      "58 batch_normalization_113\n",
      "59 activation_107\n",
      "60 activation_109\n",
      "61 activation_112\n",
      "62 activation_113\n",
      "63 mixed1\n",
      "64 conv2d_117\n",
      "65 batch_normalization_117\n",
      "66 activation_117\n",
      "67 conv2d_115\n",
      "68 conv2d_118\n",
      "69 batch_normalization_115\n",
      "70 batch_normalization_118\n",
      "71 activation_115\n",
      "72 activation_118\n",
      "73 average_pooling2d_12\n",
      "74 conv2d_114\n",
      "75 conv2d_116\n",
      "76 conv2d_119\n",
      "77 conv2d_120\n",
      "78 batch_normalization_114\n",
      "79 batch_normalization_116\n",
      "80 batch_normalization_119\n",
      "81 batch_normalization_120\n",
      "82 activation_114\n",
      "83 activation_116\n",
      "84 activation_119\n",
      "85 activation_120\n",
      "86 mixed2\n",
      "87 conv2d_122\n",
      "88 batch_normalization_122\n",
      "89 activation_122\n",
      "90 conv2d_123\n",
      "91 batch_normalization_123\n",
      "92 activation_123\n",
      "93 conv2d_121\n",
      "94 conv2d_124\n",
      "95 batch_normalization_121\n",
      "96 batch_normalization_124\n",
      "97 activation_121\n",
      "98 activation_124\n",
      "99 max_pooling2d_7\n",
      "100 mixed3\n",
      "101 conv2d_129\n",
      "102 batch_normalization_129\n",
      "103 activation_129\n",
      "104 conv2d_130\n",
      "105 batch_normalization_130\n",
      "106 activation_130\n",
      "107 conv2d_126\n",
      "108 conv2d_131\n",
      "109 batch_normalization_126\n",
      "110 batch_normalization_131\n",
      "111 activation_126\n",
      "112 activation_131\n",
      "113 conv2d_127\n",
      "114 conv2d_132\n",
      "115 batch_normalization_127\n",
      "116 batch_normalization_132\n",
      "117 activation_127\n",
      "118 activation_132\n",
      "119 average_pooling2d_13\n",
      "120 conv2d_125\n",
      "121 conv2d_128\n",
      "122 conv2d_133\n",
      "123 conv2d_134\n",
      "124 batch_normalization_125\n",
      "125 batch_normalization_128\n",
      "126 batch_normalization_133\n",
      "127 batch_normalization_134\n",
      "128 activation_125\n",
      "129 activation_128\n",
      "130 activation_133\n",
      "131 activation_134\n",
      "132 mixed4\n",
      "133 conv2d_139\n",
      "134 batch_normalization_139\n",
      "135 activation_139\n",
      "136 conv2d_140\n",
      "137 batch_normalization_140\n",
      "138 activation_140\n",
      "139 conv2d_136\n",
      "140 conv2d_141\n",
      "141 batch_normalization_136\n",
      "142 batch_normalization_141\n",
      "143 activation_136\n",
      "144 activation_141\n",
      "145 conv2d_137\n",
      "146 conv2d_142\n",
      "147 batch_normalization_137\n",
      "148 batch_normalization_142\n",
      "149 activation_137\n",
      "150 activation_142\n",
      "151 average_pooling2d_14\n",
      "152 conv2d_135\n",
      "153 conv2d_138\n",
      "154 conv2d_143\n",
      "155 conv2d_144\n",
      "156 batch_normalization_135\n",
      "157 batch_normalization_138\n",
      "158 batch_normalization_143\n",
      "159 batch_normalization_144\n",
      "160 activation_135\n",
      "161 activation_138\n",
      "162 activation_143\n",
      "163 activation_144\n",
      "164 mixed5\n",
      "165 conv2d_149\n",
      "166 batch_normalization_149\n",
      "167 activation_149\n",
      "168 conv2d_150\n",
      "169 batch_normalization_150\n",
      "170 activation_150\n",
      "171 conv2d_146\n",
      "172 conv2d_151\n",
      "173 batch_normalization_146\n",
      "174 batch_normalization_151\n",
      "175 activation_146\n",
      "176 activation_151\n",
      "177 conv2d_147\n",
      "178 conv2d_152\n",
      "179 batch_normalization_147\n",
      "180 batch_normalization_152\n",
      "181 activation_147\n",
      "182 activation_152\n",
      "183 average_pooling2d_15\n",
      "184 conv2d_145\n",
      "185 conv2d_148\n",
      "186 conv2d_153\n",
      "187 conv2d_154\n",
      "188 batch_normalization_145\n",
      "189 batch_normalization_148\n",
      "190 batch_normalization_153\n",
      "191 batch_normalization_154\n",
      "192 activation_145\n",
      "193 activation_148\n",
      "194 activation_153\n",
      "195 activation_154\n",
      "196 mixed6\n",
      "197 conv2d_159\n",
      "198 batch_normalization_159\n",
      "199 activation_159\n",
      "200 conv2d_160\n",
      "201 batch_normalization_160\n",
      "202 activation_160\n",
      "203 conv2d_156\n",
      "204 conv2d_161\n",
      "205 batch_normalization_156\n",
      "206 batch_normalization_161\n",
      "207 activation_156\n",
      "208 activation_161\n",
      "209 conv2d_157\n",
      "210 conv2d_162\n",
      "211 batch_normalization_157\n",
      "212 batch_normalization_162\n",
      "213 activation_157\n",
      "214 activation_162\n",
      "215 average_pooling2d_16\n",
      "216 conv2d_155\n",
      "217 conv2d_158\n",
      "218 conv2d_163\n",
      "219 conv2d_164\n",
      "220 batch_normalization_155\n",
      "221 batch_normalization_158\n",
      "222 batch_normalization_163\n",
      "223 batch_normalization_164\n",
      "224 activation_155\n",
      "225 activation_158\n",
      "226 activation_163\n",
      "227 activation_164\n",
      "228 mixed7\n",
      "229 conv2d_167\n",
      "230 batch_normalization_167\n",
      "231 activation_167\n",
      "232 conv2d_168\n",
      "233 batch_normalization_168\n",
      "234 activation_168\n",
      "235 conv2d_165\n",
      "236 conv2d_169\n",
      "237 batch_normalization_165\n",
      "238 batch_normalization_169\n",
      "239 activation_165\n",
      "240 activation_169\n",
      "241 conv2d_166\n",
      "242 conv2d_170\n",
      "243 batch_normalization_166\n",
      "244 batch_normalization_170\n",
      "245 activation_166\n",
      "246 activation_170\n",
      "247 max_pooling2d_8\n",
      "248 mixed8\n",
      "249 conv2d_175\n",
      "250 batch_normalization_175\n",
      "251 activation_175\n",
      "252 conv2d_172\n",
      "253 conv2d_176\n",
      "254 batch_normalization_172\n",
      "255 batch_normalization_176\n",
      "256 activation_172\n",
      "257 activation_176\n",
      "258 conv2d_173\n",
      "259 conv2d_174\n",
      "260 conv2d_177\n",
      "261 conv2d_178\n",
      "262 average_pooling2d_17\n",
      "263 conv2d_171\n",
      "264 batch_normalization_173\n",
      "265 batch_normalization_174\n",
      "266 batch_normalization_177\n",
      "267 batch_normalization_178\n",
      "268 conv2d_179\n",
      "269 batch_normalization_171\n",
      "270 activation_173\n",
      "271 activation_174\n",
      "272 activation_177\n",
      "273 activation_178\n",
      "274 batch_normalization_179\n",
      "275 activation_171\n",
      "276 mixed9_0\n",
      "277 concatenate_3\n",
      "278 activation_179\n",
      "279 mixed9\n",
      "280 conv2d_184\n",
      "281 batch_normalization_184\n",
      "282 activation_184\n",
      "283 conv2d_181\n",
      "284 conv2d_185\n",
      "285 batch_normalization_181\n",
      "286 batch_normalization_185\n",
      "287 activation_181\n",
      "288 activation_185\n",
      "289 conv2d_182\n",
      "290 conv2d_183\n",
      "291 conv2d_186\n",
      "292 conv2d_187\n",
      "293 average_pooling2d_18\n",
      "294 conv2d_180\n",
      "295 batch_normalization_182\n",
      "296 batch_normalization_183\n",
      "297 batch_normalization_186\n",
      "298 batch_normalization_187\n",
      "299 conv2d_188\n",
      "300 batch_normalization_180\n",
      "301 activation_182\n",
      "302 activation_183\n",
      "303 activation_186\n",
      "304 activation_187\n",
      "305 batch_normalization_188\n",
      "306 activation_180\n",
      "307 mixed9_1\n",
      "308 concatenate_4\n",
      "309 activation_188\n",
      "310 mixed10\n",
      "311 global_average_pooling2d_2\n",
      "312 dense_2\n"
     ]
    }
   ],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453009676890274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89       134\n",
      "           1       0.82      0.89      0.85        91\n",
      "\n",
      "    accuracy                           0.88       225\n",
      "   macro avg       0.87      0.88      0.87       225\n",
      "weighted avg       0.88      0.88      0.88       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#score = model.predict_generator(validation_generator)\n",
    "#labels = validation_generator.classes\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = model.predict(validation_generator)\n",
    "labels = validation_generator.classes\n",
    "#plot_roc(labels, score.ravel())\n",
    "auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "print(auc)\n",
    "print (classification_report(labels, score.ravel()>=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data and test the classifier\n",
    "test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "test_data = test_data/255\n",
    "test_scores    = model.predict_proba(test_data)\n",
    "\n",
    "# Save the predictions to a CSV file for upload to Kaggle\n",
    "submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores.ravel()})\n",
    "submission_file.to_csv('CNN_vanilla_0945.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.5650166],\n",
       "       [0.5650166, 1.       ]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peer = pd.read_csv('submission_PCA_SVM_3C.csv')\n",
    "peer = pd.read_csv('Inception_SVM.csv')\n",
    "#peer = pd.read_csv('CNN_vanilla3.csv')\n",
    "np.corrcoef(test_scores.ravel(),np.array(peer.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_487 (Conv2D)          (None, 96, 96, 8)         872       \n",
      "_________________________________________________________________\n",
      "batch_normalization_485 (Bat (None, 96, 96, 8)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 32, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_488 (Conv2D)          (None, 27, 27, 12)        3468      \n",
      "_________________________________________________________________\n",
      "batch_normalization_486 (Bat (None, 27, 27, 12)        48        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 9, 9, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_489 (Conv2D)          (None, 6, 6, 16)          3088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_487 (Bat (None, 6, 6, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,637\n",
      "Trainable params: 7,565\n",
      "Non-trainable params: 72\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
