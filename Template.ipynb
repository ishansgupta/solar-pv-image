{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-07c2a5b818ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_roc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Sample script for solar array image classification\n",
    "\n",
    "Author:       Kyle Bradbury, Ethan Swartzentruber\n",
    "Date:         January 30, 2018\n",
    "Organization: Duke University Energy Initiative\n",
    "'''\n",
    "\n",
    "'''\n",
    "Import the packages needed for classification\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from lib.plot_roc import plot_roc\n",
    "import datetime\n",
    "plt.close()\n",
    "\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = './data/training/'\n",
    "dir_test_images   = './data/testing/'\n",
    "dir_train_labels  = './data/labels_training.csv'\n",
    "dir_test_ids      = './data/sample_submission.csv'\n",
    "\n",
    "'''\n",
    "Include the functions used for loading, preprocessing, features extraction, \n",
    "classification, and performance evaluation\n",
    "'''\n",
    "\n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    '''\n",
    "    Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids\n",
    "\n",
    "def preprocess_and_extract_features(data):\n",
    "    '''\n",
    "    Preprocess data and extract features\n",
    "    \n",
    "    This is for anything you want to run between loading the data and calling fit.\n",
    "    See the cv_performance_assessment code for how this is used.\n",
    "    '''\n",
    "    # Here, we do something trivially simple: we take the average of the RGB\n",
    "    # values to produce a grey image, transform that into a vector, then\n",
    "    # extract the mean and standard deviation as features.\n",
    "    \n",
    "    # Make the image grayscale\n",
    "    data = np.mean(data, axis=3)\n",
    "    \n",
    "    # Vectorize the grayscale matrices\n",
    "    vectorized_data = data.reshape(data.shape[0],-1)\n",
    "    \n",
    "    # extract the mean and standard deviation of each sample as features\n",
    "    feature_mean = np.mean(vectorized_data,axis=1)\n",
    "    feature_std  = np.std(vectorized_data,axis=1)\n",
    "    \n",
    "    # Combine the extracted features into a single feature vector\n",
    "    features = np.stack((feature_mean,feature_std),axis=-1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def cv_performance_assessment(X,y,k,clf):\n",
    "    '''\n",
    "    Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        # Train the classifier\n",
    "        X_train_features = preprocess_and_extract_features(X_train)\n",
    "        clf              = clf.fit(X_train_features,y_train)\n",
    "        \n",
    "        # Test the classifier on the validation data for this fold\n",
    "        X_val_features   = preprocess_and_extract_features(X_val)\n",
    "        cpred            = clf.predict_proba(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "    return prediction_scores\n",
    "\n",
    "'''\n",
    "Function for producing a Kaggle submission\n",
    "'''\n",
    "def produce_submission(clf, fileName):\n",
    "    # Load data, extract features, and train the classifier on the training data\n",
    "    training_data, training_labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "    training_features              = preprocess_and_extract_features(training_data)\n",
    "    clf.fit(training_features,training_labels)\n",
    "\n",
    "    # Load the test data and test the classifier\n",
    "    test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "    test_features  = preprocess_and_extract_features(test_data)\n",
    "    test_scores    = clf.predict_proba(test_features)[:,1]\n",
    "\n",
    "    # Save the predictions to a CSV file for upload to Kaggle\n",
    "    submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores})\n",
    "    submission_file.to_csv(fileName, columns=['id','score'], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "training_data, training_labels = load_data(dir_train_images, dir_train_labels, training=True)\n",
    "\n",
    "# If you need training-data-dependent transformation, define variables here and reference them\n",
    "# In the preprocessing function in the first cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct classifier\n",
    "clf_name = \"KNN7\" # Used to name the submission file\n",
    "clf = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation and check the roc curve\n",
    "\n",
    "# Set parameters for the analysis\n",
    "num_training_folds = 10\n",
    "\n",
    "# Perform cross validated performance assessment\n",
    "prediction_scores = cv_performance_assessment(training_data,training_labels,num_training_folds,clf)\n",
    "\n",
    "# Compute and plot the ROC curves\n",
    "plot_roc(training_labels, prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce submission (by default is commented out)\n",
    "\n",
    "time = datetime.datetime.now().strftime(\"%m_%d_%Y_%H:%M:%S\")\n",
    "#produce_submission(clf, clf_name+\"_\"+time+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
