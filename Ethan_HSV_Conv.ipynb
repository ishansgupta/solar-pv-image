{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Convolution2D, BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import PIL\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_width = 101\n",
    "img_height = 101\n",
    "batch_size = 64\n",
    "keras = tf.keras\n",
    "\n",
    "IMG_SHAPE = (img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, prediction_scores, pos_label=1)\n",
    "    auc = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = 'AUC = {:0.3f}'.format(auc)\n",
    "   \n",
    "    plt.plot([0,1],[0,1],'--', color='gray', label='Chance')\n",
    "    plt.plot(fpr, tpr, label=legend_string)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid('on')\n",
    "    plt.axis('square')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def load_data(dir_data, dir_labels, training=True):\n",
    "    ''' Load each of the image files into memory \n",
    "\n",
    "    While this is feasible with a smaller dataset, for larger datasets,\n",
    "    not all the images would be able to be loaded into memory\n",
    "\n",
    "    When training=True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    else:\n",
    "        return data, ids\n",
    "    \n",
    "def cv_performance_assessment(X,y,k,clf):\n",
    "    '''Cross validated performance assessment\n",
    "    \n",
    "    X   = training data\n",
    "    y   = training labels\n",
    "    k   = number of folds for cross validation\n",
    "    clf = classifier to use\n",
    "    \n",
    "    Divide the training data into k folds of training and validation data. \n",
    "    For each fold the classifier will be trained on the training data and\n",
    "    tested on the validation data. The classifier prediction scores are \n",
    "    aggregated and output\n",
    "    '''\n",
    "    # Establish the k folds\n",
    "    prediction_scores = np.empty(y.shape[0],dtype='object')\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        # Extract the training and validation data for this fold\n",
    "        X_train, X_val   = X[train_index], X[val_index]\n",
    "        y_train          = y[train_index]\n",
    "        \n",
    "        # Train the classifier\n",
    "        X_train_features = X_train\n",
    "        clf              = clf.fit(X_train_features,y_train)\n",
    "        \n",
    "        # Test the classifier on the validation data for this fold\n",
    "        X_val_features   = X_val\n",
    "        cpred            = clf.predict_proba(X_val_features)\n",
    "        \n",
    "        # Save the predictions for this fold\n",
    "        prediction_scores[val_index] = cpred[:,1]\n",
    "    return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = './data/training/'\n",
    "dir_test_images   = './data/testing/'\n",
    "dir_train_labels  = './data/labels_training.csv'\n",
    "dir_test_ids      = './data/sample_submission.csv'\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, vertical_flip = True, \n",
    "                                   horizontal_flip=True, channel_shift_range=50.0,\n",
    "                                   rotation_range = 30, shear_range = 10.0,\n",
    "                                   validation_split = 0.15)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1275 validated image filenames belonging to 2 classes.\n",
      "Found 225 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate image label dataframe\n",
    "# Dont run this\n",
    "traindf = pd.read_csv(\"./data/labels_training.csv\",dtype=str)\n",
    "def append_ext(fn):\n",
    "    return fn+\".tif\"\n",
    "traindf[\"id\"]=traindf[\"id\"].apply(append_ext)\n",
    "traindf = traindf.sample(frac=1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=dir_train_images, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=12,\n",
    "    batch_size = batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe( \n",
    "    dataframe=traindf,\n",
    "    directory=dir_train_images, \n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    seed=12,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle = False,\n",
    "    batch_size = 225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Vanilla ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "\n",
    "def hsv_conversion(x):\n",
    "    import tensorflow as tf    \n",
    "    return tf.image.rgb_to_hsv(x)\n",
    "\n",
    "\n",
    "def CNN_mod2(lr, weight, name, epoch = 100):\n",
    "    # create the base pre-trained model\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Conversion from RGB to CSV\n",
    "    model.add(Lambda(hsv_conversion, input_shape=IMG_SHAPE))\n",
    "\n",
    "    # First layer\n",
    "    model.add(Convolution2D(filters = 4, kernel_size = (3, 3), \n",
    "                            input_shape = IMG_SHAPE, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Second layer\n",
    "    model.add(Convolution2D(8, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "    \n",
    "    # Third layer\n",
    "    model.add(Convolution2D(16, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Fourth layer\n",
    "    model.add(Convolution2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Fifth layer\n",
    "    model.add(Convolution2D(48, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    " \n",
    "    # Sixth layer\n",
    "    #model.add(Convolution2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "    # Flatten\n",
    "    #model.add(GlobalAveragePooling2D())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # FC\n",
    "    # model.add(Dense(units = 16, activation = 'sigmoid'))\n",
    "    \n",
    "    # Output \n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer=adam(learning_rate=lr, beta_1=0.9, beta_2=0.999), \n",
    "                  loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    filename = '/Users/ethan/solar-pv-image/' +name+ '.csv'\n",
    "    csv_logger = CSVLogger(filename, append=True, separator=';')\n",
    "    \n",
    "    model.fit_generator(train_generator, validation_data=validation_generator ,\n",
    "                        epochs=epoch, class_weight=weight)\n",
    "    \n",
    "    score = model.predict(train_generator)\n",
    "    labels = train_generator.classes\n",
    "    auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "    return (model, score, labels, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 25s 1s/step - loss: 6.1476 - auc: 0.4973 - val_loss: 0.6786 - val_auc: 0.5902\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.5871 - auc: 0.6283 - val_loss: 0.8006 - val_auc: 0.6449\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9689 - auc: 0.6641 - val_loss: 0.7685 - val_auc: 0.6764\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.0378 - auc: 0.6880 - val_loss: 0.8341 - val_auc: 0.6923\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9509 - auc: 0.7057 - val_loss: 1.0963 - val_auc: 0.7099\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9341 - auc: 0.7196 - val_loss: 1.9171 - val_auc: 0.7177\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.0834 - auc: 0.7239 - val_loss: 1.9256 - val_auc: 0.7206\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8969 - auc: 0.7273 - val_loss: 3.1867 - val_auc: 0.7243\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9803 - auc: 0.7296 - val_loss: 2.5828 - val_auc: 0.7256\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9362 - auc: 0.7296 - val_loss: 3.0126 - val_auc: 0.7278\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9808 - auc: 0.7326 - val_loss: 3.7235 - val_auc: 0.7301\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.0653 - auc: 0.7341 - val_loss: 2.2908 - val_auc: 0.7322\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.0183 - auc: 0.7360 - val_loss: 4.4388 - val_auc: 0.7345\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9543 - auc: 0.7378 - val_loss: 4.2755 - val_auc: 0.7359\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.0281 - auc: 0.7394 - val_loss: 5.2528 - val_auc: 0.7376\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8630 - auc: 0.7410 - val_loss: 4.4643 - val_auc: 0.7399\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9724 - auc: 0.7425 - val_loss: 4.0487 - val_auc: 0.7411\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8396 - auc: 0.7440 - val_loss: 4.0128 - val_auc: 0.7430\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8445 - auc: 0.7466 - val_loss: 3.1019 - val_auc: 0.7462\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8451 - auc: 0.7494 - val_loss: 1.7028 - val_auc: 0.7506\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8561 - auc: 0.7532 - val_loss: 1.7978 - val_auc: 0.7543\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8748 - auc: 0.7561 - val_loss: 1.6910 - val_auc: 0.7572\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.8791 - auc: 0.7593 - val_loss: 2.2119 - val_auc: 0.7599\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9274 - auc: 0.7622 - val_loss: 1.3952 - val_auc: 0.7640\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9641 - auc: 0.7661 - val_loss: 1.4611 - val_auc: 0.7680\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9637 - auc: 0.7699 - val_loss: 1.7295 - val_auc: 0.7713\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.7817 - auc: 0.7732 - val_loss: 1.0301 - val_auc: 0.7748\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9020 - auc: 0.7764 - val_loss: 1.8298 - val_auc: 0.7776\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.9068 - auc: 0.7794 - val_loss: 1.6207 - val_auc: 0.7803\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 24s 1s/step - loss: 0.7945 - auc: 0.7822 - val_loss: 1.4218 - val_auc: 0.7836\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 2142s 107s/step - loss: 0.8966 - auc: 0.7851 - val_loss: 1.0452 - val_auc: 0.7865\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 2925s 146s/step - loss: 0.8326 - auc: 0.7879 - val_loss: 1.1137 - val_auc: 0.7895\n",
      "Epoch 33/100\n",
      "14/20 [====================>.........] - ETA: 12s - loss: 0.8001 - auc: 0.7907"
     ]
    }
   ],
   "source": [
    "model, s, l, a = CNN_mod2(0.001, {0:1., 1:1.75}, name = 'Try_to_overfit_Ethan_HSV', epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, validation_data=validation_generator ,\n",
    "                        epochs=5, class_weight={0:1., 1:1.7})\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = model.predict(validation_generator)\n",
    "labels = validation_generator.classes\n",
    "plot_roc(labels, score.ravel())\n",
    "auc = metrics.roc_auc_score(labels, score.ravel())\n",
    "print(\"AUC:\", auc)\n",
    "print (classification_report(labels, score.ravel()>=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data and test the classifier\n",
    "test_data, ids = load_data(dir_test_images, dir_test_ids, training=False)\n",
    "test_data = test_data/255\n",
    "test_scores    = model.predict_proba(test_data)\n",
    "\n",
    "# Save the predictions to a CSV file for upload to Kaggle\n",
    "submission_file = pd.DataFrame({'id':    ids,\n",
    "                                   'score':  test_scores.ravel()})\n",
    "submission_file.to_csv('CNN_HSV_Ethan_3.csv',\n",
    "                           columns=['id','score'],\n",
    "                           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer = pd.read_csv('submission_PCA_SVM_3C.csv')\n",
    "peer = pd.read_csv('Inception_SVM.csv')\n",
    "peer = pd.read_csv('CNN_vanilla3.csv')\n",
    "peer = pd.read_csv('CNN_vanilla_0981.csv')\n",
    "np.corrcoef(test_scores.ravel(),np.array(peer.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
